\section{Performance Discussion}\label{sec:experiments}

In this section, we use our \ompi and \abft QR implementations to
evaluate the performance of the \cof protocol. We
use two test platforms. The first machine, ``Dancer,'' is a 16-node
cluster. All nodes are equipped with two 2.27GHz quad-core Intel E5520
CPUs, with a 20GB/s Infiniband interconnect. Solid State Drive disks are
used as the checkpoint storage media. The second system is the Kraken
supercomputer. Kraken is a Cray XT5 machine, with 9,408 compute nodes.
Each node has two Istanbul 2.6 GHz six-core AMD Opteron processors, 16
GB of memory, and are connected through the SeaStar2+ interconnect. The
scalable cluster file system ``Lustre'' is used to store checkpoints.

\subsection{MPI Library Overhead}

One of the concerns with fault tolerance is the amount of overhead
introduced by the fault tolerance management additions. Our
implementation of fault detection and notification is mostly implemented
in the non-critical ORTE runtime. Typical HPC systems
feature a separated service network (usually Ethernet based) and a
performance interconnect, hence health monitoring traffic, which happens
on the OOB service network, is physically separated from the MPI
communications, leaving no opportunity for network jitter. Changes to
MPI functions are minimal: the same condition that used to trigger
unconditional abort has been repurposed to trigger error handlers. As
expected, no impact on MPI bandwidth or latency was measured
(Infiniband and Portals results not shown for lack of space). The memory
usage of the MPI library is slightly increased, as the incarnation
number doubles the size of process names; however, this is negligible in
typical deployments.

\subsection{Failure Detection}

According to the requirement specified in Section~\ref{sec:interface}, only
in-band failure detection is required to enable \cof. Processes detecting a
failure checkpoint then exit, cascading the failure to processes communicating
with them. However, no recovery action (in particular checkpointing) can take 
place before a failure has been
notified. Thanks to asynchronous failure propagation in the
runtime, responsiveness can be greatly improved, with a high probability for the
next MPI call to detect the failures, regardless of communication pattern or
checkpoint duration.

\begin{figure}[htb]
\centering 
  \subfigure[Linear OOB Routing]{
    \label{fig:linear}
    \includegraphics[width=.43\linewidth]{figures/failure_detection_linear_errbars}}
  \hfill
  \subfigure[Binomial OOB Routing]{
    \label{fig:binomial}
    \includegraphics[width=.43\linewidth]{figures/failure_detection_binomial_errbars}}\vspace{-.2cm}
\caption{Failure detection time, sorted by process rank, depending on the OOB overlay network used for 
failure propagation.}\label{fig:detect}
\end{figure}

We designed a micro-benchmark to measure failure detection time as
experienced by MPI processes. The benchmark code synchronizes with an
MPI\_Barrier, stores the reference date, injects a failure at a specific
rank, and enters a ring algorithm until the MPI error handler stores the
detection date. The OOB routing topology used by the ORTE runtime introduces a
non-uniform distance to the failed process, hence failure detection time
experienced by a process may vary with the position of the failed
process in the topology, and the OOB topology. Figure~\ref{fig:linear}
and~\ref{fig:binomial} present the case of the linear and
binomial OOB topologies, respectively. The curves ``Low, Middle, High'' present the
behavior for failures happening at different positions in the OOB
topology. On the horizontal axis is the rank of the detecting process,
on the vertical axis is the detection time it experienced. The
experiment uses 16 nodes, with one process per node, MPI over Infiniband, OOB
over Ethernet, an average of 20 runs, and the MPI barrier latency is four orders of
magnitude lower than measured values.

In the linear topology (Figure~\ref{fig:linear}) every runtime process
is connected to the \emph{mpirun} process. For a higher rank, failure
detection time increases linearly because it is notified by the
\emph{mpirun} process only after the notification has been sent to all
lower ranks. This issue is bound to increase with scale. The binomial
tree topology (Figure~\ref{fig:binomial}) exhibits a similar best
failure detection time. However, this more scalable topology has a low
output degree and eliminates most contentions on outgoing messages,
resulting in a more stable, lower average detection time, regardless of the
failure position. Overall, failure detection time is on the order of
milliseconds, a much smaller figure than typical checkpoint time.

% WE SHOULD HAVE SOME RESULTS WITH AUTO-FT AND/OR APP BASED PERIODIC CHECKPOINT



\begin{figure}[thb]
\centering
\begin{minipage}[t]{.29\linewidth}
	\includegraphics[width=\linewidth]{figures/kraken_new_data}
	\vspace{-.6cm}\caption{ABFT QR and one \cof recovery on Kraken (Lustre).}%($24\times 24$ grid)
	\label{fig:kraken_performance}	
\end{minipage}
\hfill
\begin{minipage}[t]{.29\linewidth}
    \includegraphics[width=\linewidth]{figures/dancer_performance_data}
	\vspace{-.6cm}\caption{ABFT QR and one \cof recovery on Dancer (local SSD).} %($16\times 8$ grid)
    \label{fig:dancer_performance}
\end{minipage}
\hfill
\begin{minipage}[t]{.29\linewidth}
    \includegraphics[width=\linewidth]{figures/dancer_1_error_timing_process_new_data}
	\vspace{-.6cm}\caption{Time breakdown of one \cof recovery on Dancer (local SSD).}
    \label{fig:dancer_timing}
\end{minipage}\vspace{-.3cm}
\end{figure}

\subsection{Checkpoint-on-Failure QR Performance}

\paragraph*{Supercomputer Performance:}
Figure~\ref{fig:kraken_performance} presents the performance on the
Kraken supercomputer. The process grid is $24\times 24$ and the block size
is 100. The \cof-QR (no failure) presents the performance of the \cof QR
implementation, in a fault-free execution; it is noteworthy, that when
there are no failures, the performance is exactly identical to the
performance of the unmodified FT-QR implementation. The \cof-QR (with
failure) curves present the performance when a failure is injected after
the first step of the PDLARFB kernel. The performance of the non-fault
tolerant ScaLAPACK QR is also presented for reference.

Without failures, the performance overhead compared to the regular
ScaLAPACK is caused by the extra computation to maintain the checksums inherent to the \abft
algorithm~\cite{pengduppopp12}; this extra computation is unchanged between \cof-QR and
FT-QR. Only on runs where a failure happened do the \cof protocols
undergoe the supplementary overhead of storing and reloading
checkpoints. However, the performance of the \cof-QR remains very
close to the no-failure case. For instance, at matrix size N=100,000,
\cof-QR still achieves 2.86 Tflop/s after recovering from a failure,
which is 90\% of the performance of the non-fault tolerant ScaLAPACK QR.
This demonstrates that the \cof protocol enables efficient, practical
recovery schemes on supercomputers.

\paragraph*{Impact of Local Checkpoint Storage:}

Figure~\ref{fig:dancer_performance} presents the performance of the \cof-QR 
implementation on the Dancer cluster with a $8\times 16$ process
grid. Although a smaller test platform, the Dancer cluster features
local storage on nodes and a variety of performance analysis tools
unavailable on Kraken. As expected (see~\cite{pengduppopp12}), the \abft
method has a higher relative cost on this smaller machine. Compared to
the Kraken platform, the relative cost of \cof failure recovery is
smaller on Dancer. The \cof protocol incurs disk accesses to store and
load checkpoints when a failure hits, hence the recovery overhead
depends on I/O performance. By breaking down the relative cost of each
recovery step in \cof, Figure~\ref{fig:dancer_timing} shows that
checkpoint saving and loading only take a small percentage of the total
run-time, thanks to the availability of solid state disks on every node.
Since checkpoint reloading immediately follows checkpointing, the OS
cache satisfy most disk access, resulting in high I/O performance. For
matrices larger than N=44,000, the memory usage on each node is high and
decrease the available space for disk cache, explaining the decline in
I/O performance and the higher cost of checkpoint management. Overall,
the presence of fast local storage can be leveraged by the \cof protocol
to speedup recovery (unlike periodic checkpointing, which
depends on remote storage by construction). Nonetheless, as demonstrated by the
efficiency on Kraken, while this is a valuable optimization, it is not a
mandatory requirement for satisfactory performance.

%On a problem of this size, the additional overheads
%are dominated by the time it takes to terminate the failing MPI
%application and relaunch a new one. 
