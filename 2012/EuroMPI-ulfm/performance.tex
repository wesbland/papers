\section{Performance Analysis}
\label{sect:performance}

The following analysis used a prototype of the \ulfm proposal based on
the development trunk of Open MPI~\cite{gabriel04:_open_mpi} (r26237).
The test results presented were gathered from the Smoky system at Oak
Ridge National Laboratory. Each node contains four quad-core 2.0~GHz AMD
Opteron processors with 2 GB of memory per compute core. Compute nodes
are connected with gigabit Ethernet and InfiniBand. Some shared-memory
benchmarks were conducted on Romulus, a $6\times8$-core AMD Opteron 6180
SE with 256GB of memory (32GB per socket) at the University of Tennessee.

The NetPIPE benchmark
(v3.7) was used to assess the 1-byte latency and bandwidth
impact of the modifications necessary for the \ulfm support in Open MPI.
We compare the vanilla version of Open~MPI (r26237) with the \ulfm enabled
version on Smoky. Table~\ref{tab:netpipe} highlights the fact that the differences in
performance are well below the noise limit, and that the standard deviation is
negligible proving the performance stability and lack of impact.

\begin{table}\vspace{-.4cm}
\begin{center}\sf\scriptsize
\begin{tabular}{|l||r|r||r|r||r|}
\multicolumn{6}{c}{1-byte Latency (microseconds) (cache hot)} \\
\hline
\cellcolor[gray]{0.7}\textbf{Interconnect}  & \cellcolor[gray]{0.7}\textbf{Vanilla}   & \cellcolor[gray]{0.7}\textbf{Std. Dev.} &
\cellcolor[gray]{0.7}\textbf{Enabled}         & \cellcolor[gray]{0.7}\textbf{Std. Dev.} & \cellcolor[gray]{0.7}\textbf{Difference} \\
\hline
\cellcolor[gray]{0.9}Shared Memory &  0.8008 & 0.0093 &  0.8016 & 0.0161 &  0.0008 \\
\cellcolor[gray]{0.9}TCP           & 10.2564 & 0.0946 & 10.2776 & 0.1065 &  0.0212 \\
\cellcolor[gray]{0.9}OpenIB        &  4.9637 & 0.0018 &  4.9650 & 0.0022 &  0.0013 \\
\hline
\multicolumn{6}{c}{Bandwidth (Mbps) (cache hot)} \\
\hline
\cellcolor[gray]{0.7}\textbf{Interconnect}  & \cellcolor[gray]{0.7}\textbf{Vanilla}   & \cellcolor[gray]{0.7}\textbf{Std. Dev.} &
\cellcolor[gray]{0.7}\textbf{Enabled}         & \cellcolor[gray]{0.7}\textbf{Std. Dev.} & \cellcolor[gray]{0.7}\textbf{Difference} \\
\hline
\cellcolor[gray]{0.9}Shared Memory &  10,625.92 &  23.46 &  10,602.68 & 30.73 & -23.24 \\
\cellcolor[gray]{0.9}TCP           &   6,311.38 &  14.42 &   6,302.75 & 10.72 &  -8.63 \\
\cellcolor[gray]{0.9}OpenIB        &   9,688.85 &   3.29 &   9,689.13 &  3.77 &   0.28 \\
\hline
\end{tabular}
\end{center}
\caption{NetPIPE results on Smoky.\label{tab:netpipe}}\vspace{-.8cm}
\end{table}

The impact on shared memory systems, which are sensitive even to small
modifications of the MPI library, has been further assessed on the
Romulus machine -- a large shared memory machine -- using the IMB
benchmark suite (v3.2.3). As shown in Figure~\ref{fig:IMB}, the duration
difference of all the benchmarks (point-to-point and collective) remains
below 5\%, thus within the standard deviation of the implementation on
that machine.

\begin{figure}
\begin{center}\vspace{-.6cm}
 \includegraphics[width=7cm]{figures/IMB.pdf}\vspace{-.4cm}
 \caption{The Intel MPI Benchmarks: relative difference
   between \ulfm and the vanilla Open MPI on shared memory
  (Romulus). Standard deviation $\approx$5\% on 1,000 runs.\label{fig:IMB}}\vspace{-.6cm}
\end{center}
\end{figure}

To measure the impact of the prototype on a real application, we used the
Sequoia AMG benchmark\footnote{https://asc.llnl.gov/sequoia/benchmarks/\#amg}.
This MPI intensive benchmark is an Algebraic Mult-Grid (AMG) linear system
solver for unstructured mesh physics. A weak scaling study was conducted up to
512 processes following the problem \emph{Set 5}. In
Figure~\ref{fig:sequoia:bargraph}, we compare the time slicing of three main
phases (Solve, Setup, and SStruct) of the benchmark, with, side by side, the
vanilla version of the Open MPI implementation, and the \ulfm enabled one. The
application itself is not fault tolerant and does not use the features proposed
in \ulfm. The goal of this benchmark is to demonstrate that a careful
implementation of the proposed semantic does not impact the performance of the
MPI implementation, and ultimately leaves the behavior and performance
of legacy applications unchanged. The results show that the performance 
difference is negligible.

\begin{figure}
  \begin{minipage}[t]{0.48\linewidth}
    \includegraphics[width=\linewidth]{figures/bargraph.pdf}\vspace{-.4cm}
    \caption{Comparison of the vanilla and \ulfm versions of Open MPI running
      Sequoia-AMG at different scales (Smoky).\label{fig:sequoia:bargraph}}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.48\linewidth}
    \includegraphics[width=\linewidth]{figures/scalability.pdf}\vspace{-.4cm}
    \caption{Evaluation of the Fault Injection Benchmark with full
      recovery at different scales (Smoky).\label{fig:frssj:scalability}}
  \end{minipage}\vspace{-.5cm}
\end{figure}

To assess the overheads of recovery constructs, we developed a synthetic
benchmark that mimics the behavior of a typical fixed-size tightly-coupled
fault-tolerant application. Unlike a normal application it performs an infinite
loop, where each iteration contains a failure and the corresponding recovery
procedure. Each iteration consists of 5 phases: in the first phase
(\emph{Detection}), all processes but a designated victim enter a Barrier on the
intracommunicator. The victim dies, and the failure detection mechanism makes
all surviving processes exit the Barrier, some with an error code. In Phase 2
(\emph{Revoke}), the surviving processes that detected a process-failure related
error during the previous phase invoke the new construct
\mpifunc{MPI\_COMM\_REVOKE}. Then they proceed to Phase 3 (\emph{Shrink}) where
the intracommunicator is shrunk using \mpifunc{MPI\_COMM\_SHRINK}. The two
other phases serve to repair a full-size intracommunicator using 
spawn and intercommunicator merge operations to allow the
benchmark to proceed to the next round.

In Figure~\ref{fig:frssj:scalability}, we present the timing of each phase,
averaged upon 50 iterations of the benchmark loop, for a varying number of
processes on the Smoky machine. We focus on the three points related to \ulfm:
failure detection, revoke and shrink. The failure detection is mildly impacted by
the scale. In the prototype implementation, the detection happens at two levels,
either in the runtime system or in the MPI library (when it occurs on an active
link).
Between the two detectors, all ranks get notified within 30ms of the
failure (this compares to the 1s timeout at the link level). Although the revoke call will inject a linear number of messages (at
each rank) in the network to implement the level of reliability required for
this operation, the duration of this call itself is under 50$\mu$s and is not
visible in the figure. The network is disturbed for a longer period, due to the
processing of the messages, but this disturbance will appear in the network only
after a failure occurred.  The last call shown in the figure is the shrink
operation. Although its duration increases linearly with the number of processes
(the figure has a logarithmic scale on the x-axis), this cost must only be paid
after a failure, in order to continue using collective operations. In its
current implementation, shrink requires an agreement, the allocation of a new
communicator identifier, and the creation of the communicator (with
\mpifunc{MPI\_COMM\_SPLIT}). Most of the time spent in the shrink operation is 
not in the agreement (which scales logarithmically), but in the underlying 
implementation of the communicator creation. 
