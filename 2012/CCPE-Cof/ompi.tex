\section{Enabling Algorithm-Based Fault Tolerance in MPI}
\label{sect:ompi}

\subsection{The Checkpoint-on-Failure Protocol}

In this paper, we advocate that an extremely efficient form of fault
tolerance can be implemented, strictly based on the MPI Standard, for
applications capable of taking advantage of forward recovery. \abft
methods are a family of forward recovery algorithms, capable of
restoring missing data from redundant information located on other
processes. In the remainder of this text, we will consider the case 
of \abft without loss of generality: any other forward
recovery technique could be substituted. Forward recovery requires that
communication between processes can resume, and we acknowledge that, in
light of the current standard, requiring the MPI implementation to
maintain service after failures is too demanding. However, a
high-quality MPI library should at least allow the application to regain
control following a process failure. We note that this control gives the
application the opportunity to save its state and exit gracefully,
rather than the usual behavior of being aborted by the MPI
implementation.

\begin{algorithm}
\caption{The Checkpoint-on-Failure Protocol}
\label{alg:on-demand-alg}\label{fig:idea}
\centering
\begin{minipage}[b]{.55\linewidth}
	\begin{enumerate}\sffamily\small
		\item MPI returns an error on surviving processes 
		\item Surviving processes checkpoint
		\item Surviving processes exit
		\item A new MPI application is started
		\item Processes load from checkpoint (if any)
		\item Processes enter \abft dataset recovery
		\item Application resumes
	\end{enumerate}
\vfill
\end{minipage}
\hfill
\begin{minipage}[b]{.44\linewidth}
\includegraphics[width=\linewidth]{figures/idea.pdf}
%\caption{Failure recovery execution diagram of On-Demand Checkpointing\label{fig:idea}}
\end{minipage}
\end{algorithm}

Based on these observations, we propose a new approach for supporting
application based forward recovery, called Checkpoint-on-Failure (\cof).
Algorithm~\ref{alg:on-demand-alg} presents the steps involved in the
\cof method. In the associated explanatory figure, horizontal lines
represent the execution of processes in two successive MPI applications.
When a failure eliminates a process, other processes are notified and
regain control from ongoing MPI calls (1). Surviving processes assume
the MPI library is dysfunctional and do not call further MPI operations
(in particular, they do not yet undergo \abft recovery). Instead, they
checkpoint their current state independently and abort (2, 3). When
all processes exited, the job is usually terminated, but the user (or a
managing script, batch scheduler, runtime support system, etc.) can
launch a new MPI application (4), which reloads the dataset from the 
checkpoint (5). In the new application, the MPI library is functional
and communications possible; the \abft recovery procedure is called to
restore the data of the process(es) that could not be restarted from
checkpoint (6). When the global state has been repaired by the \abft
procedure, the application is ready to resume normal execution.

%AURELIEN: true enough, but I need to  cut lines. 
%If another failure hits the system during the recovery, the local states
%are not updated, and the relaunch starts from the beginning. If another
%failure hits the system after the \abft recovery, the same procedure is
%followed to handle it.

Compared to periodic checkpointing, in \cof, a process pays the cost of
creating a checkpoint only when a failure, or multiple simultaneous
failures have happened, hence an optimal number of checkpoints during
the run (and no checkpoint overhead on failure-free executions).
Moreover, in periodic checkpointing, a process is protected only when
its checkpoint is stored on safe, remote storage, while in \cof, local
checkpoints are sufficient: the forward recovery algorithm reconstructs
datasets of processes which cannot restart from checkpoint. Of course,
\cof also exhibits the same overhead as the recovery technique employed
by the application. In an \abft approach, the application might need to
do extra computation, even in the absence of failures, to maintain
internal redundancy (whose degree varies with the maximum number of
simultaneous failures) used to recover data damaged by failures.
However, \abft techniques often demonstrate excellent scalability; for
example, the overhead on failure-free execution of the \abft QR
operation (used as an example in Section~\ref{sec:ftla}) is inversely
proportional to the number of processes~\cite{pengduppopp12}. 

\subsection{MPI Requirements for Checkpoint-on-Failure}\label{sec:interface}

\paragraph*{Returning Control over Failures:} In most MPI
implementations, MPI\_ERRORS\_ABORT is the default (and often, only
functional) error handler. However, the MPI Standard also defines the
MPI\_ERRORS\_RETURN handler. To support \cof, the MPI library should
never deadlock because of failures, but invoke the error handler, at
least on processes doing direct communications with the failed process, 
and returns control to the application.

\paragraph*{Preparing for Checkpoint:} If the MPI implementation intends
to support a system based checkpoint module (such as MTCP~\cite{rieker:pdpta:2006}),
before invoking the error handler, the MPI library must dispose of its
internal state, so that it is not restored by the checkpoint library
upon restart. This cleanup consists in releasing all acquired
resources and freeing internal buffers and structures, which is safe as
the MPI library is not supposed to be functional anymore. If the MPI
implementation intends to support only user directed checkpoint, this
effort can be spared.

\paragraph*{Termination After Checkpoint:} A process that detects a
failure ceases to use MPI. It only checkpoints on some storage and exits
without calling MPI\_Finalize. Exiting without calling MPI\_Finalize is
an error from the MPI perspective, hence the failure cascades following
the communication pattern of the application, and MPI eventually returns
with a failure notification on every process, which triggers their own
checkpoint procedure and termination. Only processes that do not
communicate may reach MPI\_Finalize without detecting a failure, adding
a Barrier before MPI\_Finalize will result in the expected error in that
case.

\subsection{\ompi Implementation\label{sec:mpi}}

\ompi is an MPI 2.2 implementation architected such that it contains two
main levels, the runtime (ORTE) and the MPI library (OMPI). As with most
MPI library implementations, the default behavior of \ompi is to abort
after a process failure. This policy was implemented in the runtime
system, preventing any kind of decision from the MPI layer or the
user-level. The major change requested by the \cof protocol was to make the
runtime system resilient, and leave the decision in case of failure to
the MPI library policy, and ultimately to the user application.

\paragraph*{Failure Resilient Runtime:} For full support of the \cof
protocol, it is sufficient for the runtime to delay the cleanup of the
failed MPI application until it terminates itself. However, a persistent
runtime that remains available for spawning replacement MPI processes
confers a number of advantages compared to this simple design. Indeed,
it eliminates the downtime resulting from the complete redeployment of
the parallel job infrastructure and the supplementary wait-time from
losing the batch scheduler reservation. In addition, it can serve as a
local storage for checkpoints.

The ORTE runtime layer depends on an out-of-band communication mechanism
(OOB); therefore, node failures not only impact the MPI communications,
but also disrupt the OOB overlay routing topology. Fortunately,
restoring TCP based OOB communications is easier than it is to repair
MPI. The default routing policy in the Open MPI runtime has been amended
to allow for self-healing behaviors. The OOB overlay topology now
automatically routes around failed processes. In some routing
topologies, such as a star, this is a trivial operation and only
requires excluding the failed process from the routing tables. For more
elaborate topologies, such as a binomial tree, the healing operation
involves computing the closest neighbors in the direction of the failed
process and reconnecting the topology through them. The repaired
topology is not rebalanced, resulting in degraded performance but
complete functionality after failures. Although in-flight messages that
were currently ``hopping'' through the failed processes are lost, newer
in-flight messages are safely routed on the repaired topology. Thanks to
self-healing topologies, the runtime remains responsive and can continue
to support the replacement MPI application.

\paragraph*{Failure Notification:} Although not strictly necessary to 
support \cof, rapid dissemination of failure notifications has a significant 
influence on the delay before the recovery can start. The runtime has 
therefore been augmented with a failure detection and dissemination 
service. To track the status of the failures, an
incarnation number has been included in the process names. Following a
failure, the name of the failed process (including the incarnation
number) is broadcasted over the OOB topology. By including this
incarnation number, we can identify transient process failures, prevent
duplicate detections, and track message status. ORTE processes monitor
the health of their neighbors in the OOB routing topology. Detection of
other processes rely on a failure resilient broadcast that overlays on
the OOB topology. This broadcast algorithm has a low probability of creating a
bi-partition of the routing topology, hence ensuring a high accuracy of
the failure detector. We will show in the experiments that the underlying 
OOB routing algorithm has a significant influence on the propagation
time. Finally, on each node, the ORTE runtime layer forwards
failure notifications to the MPI layer, which has been modified to
invoke the appropriate MPI error handler.
%(MPI\_ERR\_RETURNS, or a custom error handler). MPI calls are interrupted and
%the application regains control. The MPI library purges internal queues and
%state, to minimize the size of checkpoint.
