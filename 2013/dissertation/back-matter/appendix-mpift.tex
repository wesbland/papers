%
% Ticket 323
% https://svn.mpi-forum.org/trac/mpi-forum-web/ticket/323
%
% Based on the wiki page
% https://svn.mpi-forum.org/trac/mpi-forum-web/wiki/User_Level_Failure_Mitigation
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand*\descriptionlabel[1]{\hspace\labelsep\it #1}

\appendixchapter{Process Fault Tolerance}
\label{chap:ft}
\subsubsection*{Chapter Submitted to MPI Forum. Section references not preceded by an \texttt{A} refer to the MPI 3.0 Standard document.}

\appendixsection{Introduction}
\label{sec:ft-intro}

Long running and large scale applications are at increased risk of encountering
process failures during normal execution. We consider a
process failure as a fail-stop failure; failed processes become permanently
unresponsive to communications.  This chapter introduces the \mpi features
that support the development of applications and libraries that can tolerate
process failures.  The approach described in this chapter is intended to prevent
the deadlock of processes while avoiding impact on the failure-free execution of
an application.

The expected behavior of \mpi in the case of a process failure is defined
by the following statements: any \mpi operation that involves a failed process
must not block indefinitely, but either succeed or raise an \mpi exception (see
Section~\ref{sec:ft-notification}); an \mpi operation that does not involve the
failed process will complete normally, unless interrupted by the user through
provided functionality. Asynchronous failure propagation is not required. If an
application needs global knowledge of failures, it can use the interfaces
defined in Section~\ref{sec:ft-functions} to explicitly propagate locally
detected failures.  

An implementation that does not tolerate process failures must provide the
interfaces and semantics defined in this chapter as long as no failure 
occurred. It must never raise an exception of class 
\mpifunc{MPI\_ERR\_PROC\_FAILED} or \mpifunc{MPI\_ERR\_PENDING}
because of a process failure.
This chapter does not define process failure semantics for the operations
specified in Chapters 10, 11 and 12, therefore they remain undefined by the \mpi standard.

\begin{description}

\item[Advice to Users] {Many of the operations and semantics described in
this chapter are only applicable when the \mpi application has replaced
the default error handler \mpifunc{MPI\_ERRORS\_ARE\_FATAL} on, at least,
\mpifunc{MPI\_COMM\_WORLD}.}

\end{description}

\appendixsection{Failure Notification}
\label{sec:ft-notification}

This section specifies the behavior of an \mpi communication
operation when
failures occur on processes involved in the communication. A process is
considered involved in a communication if any of the following is true:

\begin{enumerate}

    \item the operation is collective and the process appears in one of the
        groups {of the associated communication object};

    \item the process is a specified or matched destination or source in a
        point-to-point communication;

	\item the operation is an \mpifunc{MPI\_ANY\_SOURCE} receive operation and the
        failed process belongs to the source group.

\end{enumerate}


Therefore, if an operation does not involve a failed process (such as a point-to-point 
message between two non-failed processes), it must not raise a process
failure exception. 

\begin{description}

\item[Advice to Implementors] {A correct \mpi implementation may provide
failure detection only for processes involved in an ongoing operation,
and postpone detection of other failures until necessary.  Moreover, as
long as an implementation can complete operations, it may choose to
delay raising an error. Another valid implementation might choose to
raise an error as quickly as possible.}

\end{description}


Non-blocking operations must not raise an exception about process failures during
initiation. All process failure errors are postponed until the corresponding
completion function is called.

\appendixsubsection{Startup and Finalize}
\label{sec:ft-notification:init-finalize}

\begin{description}

\item[Advice to Implementors] {If a process fails during
\mpifunc{MPI\_INIT} but its peers are able to complete the \mpifunc{MPI\_INIT} 
successfully, then a high quality implementation will 
return \mpifunc{MPI\_SUCCESS} and delay the reporting of the process failure 
to a subsequent \mpi operation.}

\end{description}

\mpifunc{MPI\_FINALIZE} will complete successfully even in the presence of process failures.


\begin{description}

\item[Advice to Users] {Considering Example 8.7 in Section 8.7, 
the process with rank 0 in \mpifunc{MPI\_COMM\_WORLD} may have failed before, 
during, or after the call to \mpifunc{MPI\_FINALIZE}.
\mpi only provides failure detection capabilities up to when 
\mpifunc{MPI\_FINALIZE} is invoked and provides no support for fault tolerance 
during or after \mpifunc{MPI\_FINALIZE}. Applications are encouraged to implement 
all rank-specific code before the call to \mpifunc{MPI\_FINALIZE} 
to handle the case where process 0 in \mpifunc{MPI\_COMM\_WORLD} fails.}

\end{description}

\appendixsubsection{Point-to-Point and Collective Communication}
\label{sec:ft-notification:p2p-coll-comm}

An \mpi implementation raises the following error classes to notify
users that a point-to-point communication operation could not complete
successfully because of the failure of involved processes:

\begin{itemize}

\item \mpifunc{MPI\_ERR\_PENDING} indicates, for a non-blocking communication,
   that the communication is a receive operation from 
   \mpifunc{MPI\_ANY\_SOURCE} and no matching send has been posted, yet 
    a potential sender process has failed. Neither the operation nor 
    the request identifying the operation are completed. Note that the
    same error class is also used in status when another communication
    raises an exception during the same operation (as defined in 
    Section 3.7.5).
	
\item In all other cases, the operation raises an exception of class
    \mpifunc{MPI\_ERR\_PROC\_FAILED} to indicate that the failure prevents the
    operation from following its failure-free specification. If there is a
    request identifying the point-to-point communication, it is completed.
    Future point-to-point communication with the same process on this 
    communicator must also raise \mpifunc{MPI\_ERR\_PROC\_FAILED}.

\end{itemize}

\begin{description}

\item[Advice to Users] {To acknowledge a failure and discover which processes failed, the user should
call \mpifunc{MPI\_COMM\_FAILURE\_ACK} (as defined in
Section~\ref{sec:ft-functions:commfunctions}).}

\end{description}

When a collective operation cannot be completed because of the failure of
an involved process, the collective operation raises an error of class 
\mpifunc{MPI\_ERR\_PROC\_FAILED}. 

\begin{description} 

\item[Advice to Users] {Depending on how the collective operation is implemented and when a process
failure occurs, some participating alive processes may raise an exception while
other processes return successfully from the same collective operation. For
example, in \mpifunc{MPI\_BCAST}, the root process may succeed before a failed
process disrupts the operation, resulting in some other processes raising an
error. 
%
However, it is noteworthy that for collective operations on an intracommunicator
in which all processes contribute to the result and all
processes receive the result, processes which do not enter the
operation due to process failure provoke all surviving ranks to raise
\mpifunc{MPI\_ERR\_PROC\_FAILED}. 
%
Similarly, for the same collective
operations on an intercommunicator, a process in the remote group which failed
before entering the operation has the same effect on all surviving ranks of the
local group.}

\item[Advice to Users] {Note that communicator creation functions (like \mpifunc{MPI\_COMM\_DUP} or
\mpifunc{MPI\_COMM\_SPLIT}) are collective operations. As such, if a failure
happened during the call, an error might be raised at some processes while
others succeed and obtain a new communicator.  While it is valid to communicate
between processes which succeeded to create the new communicator, it is the
responsibility of the user to ensure that all involved processes have a
consistent view of the communicator creation, if needed.
%
A conservative solution is to have each process either revoke 
(see Section~\ref{sec:ft-functions:commfunctions}) the parent communicator if the
operation fails, or call an \mpifunc{MPI\_BARRIER} on the parent communicator
and then revoke the new communicator if the \mpifunc{MPI\_BARRIER} fails.}

\end{description}

When a communication operation raises an exception related to process failure, the
content of the output buffers is \emph{undefined}.

\appendixsubsection{Dynamic Process Management}
\label{sec:ft-notification:dyn-process}


Dynamic process management functions require some additional semantics from the
\mpi implementation as detailed below.

\begin{enumerate}

    \item If the \mpi implementation raises an error related to process
        failure to the root process of \mpifunc{MPI\_COMM\_CONNECT} or
        \mpifunc{MPI\_COMM\_ACCEPT}, at least the root processes of both
        intracommunicators must raise the same error of class
        \mpifunc{MPI\_ERR\_PROC\_FAILED} (unless required to raise
        \mpifunc{MPI\_ERR\_REVOKED} as defined by~\ref{sec:ft-functions:commfunctions}).

    \item If the \mpi implementation raises an error related to
        process failure to the root process of \mpifunc{MPI\_COMM\_SPAWN}, no spawned
        processes should be able to communicate on the created
        intercommunicator.

\end{enumerate}

\begin{description}

\item[Advice to Users] {As with communicator creation functions, it is
possible that if a failure happens during dynamic process management
operations, an error might be raised at some processes while others succeed
and obtain a new communicator.}

\end{description}

\appendixsubsection{One-Sided Communication}
\label{sec:ft-notification:one-sided}


As with all nonblocking operations, one-sided communication operations should
delay all failure notification until their synchronization operations which may raise
\mpifunc{MPI\_ERR\_PROC\_FAILED} (see Section~\ref{sec:ft-notification}). If the
implementation raises an error related to process failure from the
synchronization function, the epoch behavior is unchanged from the definitions
in Section 11.4. As with collective operations over \mpi communicators, it is
possible that some processes have detected a failure and raised
\mpifunc{MPI\_ERR\_PROC\_FAILED}, while others returned \mpifunc{MPI\_SUCCESS}. 


Unless specified below, the state of memory
targeted by any process in an epoch in which operations raised an error
related to process failure is undefined.

\begin{enumerate}

    \item If a failure is to be reported during active target communication
        functions \mpifunc{MPI\_WIN\_COMPLETE} or \mpifunc{MPI\_WIN\_WAIT} (or the non-blocking
        equivalent \mpifunc{MPI\_WIN\_TEST}), the epoch is considered completed and all
        operations not involving the failed processes must complete
        successfully.

    \item If the target rank has failed, \mpifunc{MPI\_WIN\_LOCK} and \mpifunc{MPI\_WIN\_UNLOCK} 
    	operations raise an error of class \mpifunc{MPI\_ERR\_PROC\_FAILED}.
        %
        If the owner of a lock has failed, the lock cannot be acquired again,
        and all subsequent operations on the lock must raise \mpifunc{MPI\_ERR\_PROC\_FAILED}.

\end{enumerate}

\begin{description}

\item[Advice to Users] {It is possible that request-based RMA operations
complete successfully while the enclosing epoch completes by raising error due
to process failure.  In this scenario, the local buffer is valid but the
remote targeted memory is undefined.}

\end{description}

\appendixsubsection{I/O}
\label{sec:ft-notification:io}

I/O error classes and their consequences are defined in
Section 13.7. The following section
defines the behavior of I/O operations when MPI process failures prevent their
successful completion.
%
Since collective I/O operations may not synchronize with other processes,
process failures may not be reported during a collective I/O operation. If a
process failure prevents a file operation from completing, an \mpi exception of
class \mpifunc{MPI\_ERR\_PROC\_FAILED} is raised.
%
Once an \mpi implementation has raised an error of class
\mpifunc{MPI\_ERR\_PROC\_FAILED}, the state of the file pointer is
\emph{undefined}.

\begin{description}
    
\item[Advice to Users]{Users are encouraged to use \mpifunc{MPI\_COMM\_AGREE} on a communicator
containing the same group as the file handle, to deduce the completion status of
collective operations on file handles and maintain a consistent view of file
pointers.}

\end{description}

\appendixsection{Failure Mitigation Functions}
\label{sec:ft-functions}

\appendixsubsection{Communicator Functions}
\label{sec:ft-functions:commfunctions}

 \mpi provides no guarantee of global knowledge of a process failure. Only
processes involved in a communication operation with the failed process are
guaranteed to eventually detect its failure (see
Section~\ref{sec:ft-notification}). If global knowledge is required, \mpi
provides a function to revoke a communicator at
all members.

\subsubsection*{MPI\_COMM\_REVOKE( comm ) \\
\begin{tabular*}{1.0\textwidth}{@{\extracolsep{\fill}} l l c r }
  \texttt{IN} & \texttt{comm} & & communicator (handle) \\
\end{tabular*}}

This function notifies all processes in the groups (local and remote) associated
with the communicator \textit{comm} that this communicator is now considered
revoked. This function is not collective and therefore does not have a matching 
call on remote processes. It is erroneous to call
\mpifunc{MPI\_COMM\_REVOKE} on a communicator for which no operation raised an
\mpi exception related to process failure. All alive processes belonging to
\textit{comm} will be notified of the revocation despite failures. 
The revocation of a communicator completes any non-local \mpi operations on
\textit{comm} by raising an error of class \mpifunc{MPI\_ERR\_REVOKED},
with the exception of \mpifunc{MPI\_COMM\_SHRINK} and
\mpifunc{MPI\_COMM\_AGREE} (and its nonblocking equivalent). A communicator
becomes revoked as soon as:

\begin{enumerate} 
    
    \item
        \mpifunc{MPI\_COMM\_REVOKE} is locally called on it;
    
    \item Any \mpi operation raised an error of class \mpifunc{MPI\_ERR\_REVOKED}
        because another process in \textit{comm} has called \mpifunc{MPI\_COMM\_REVOKE}.
            
\end{enumerate}


Once a communicator has been revoked, all subsequent non-local operations on that
communicator, with the exception of \mpifunc{MPI\_COMM\_SHRINK} and \mpifunc{MPI\_COMM\_AGREE}
(and its nonblocking equivalent), are considered local and must
complete by raising an error of class \mpifunc{MPI\_ERR\_REVOKED}. 

\begin{description}

\item[Advice to Users] {High quality implementations are encouraged to do their best to free resources locally when the user calls free operations on revoked communication objects, or communication objects containing failed processes.}

\end{description}

\subsubsection*{MPI\_COMM\_SHRINK( comm, newcomm ) \\
\begin{tabular*}{1.0\textwidth}{@{\extracolsep{\fill}} l l c r }
  \texttt{IN} & \texttt{comm} & & communicator (handle) \\
  \texttt{OUT} & \texttt{newcomm} & & communicator (handle) \\
\end{tabular*}}

This collective operation creates a new intra or inter communicator \textit{newcomm} from
the revoked intra or inter communicator \textit{comm} respectively by
excluding its failed processes as detailed below.  It is erroneous \mpi code to
call \mpifunc{MPI\_COMM\_SHRINK} on a communicator which has not been
revoked (as defined above) and will raise an error of class \mpifunc{MPI\_ERR\_ARG}. 


%
This function must not raise an error due to process failures (error classes
\mpifunc{MPI\_ERR\_PROC\_FAILED} and \mpifunc{MPI\_ERR\_REVOKED}).  All
processes that succeeded agreed on the content of the group of processes that
failed.  This group includes at least every process failure that has raised an
\mpi exception of class \mpifunc{MPI\_ERR\_PROC\_FAILED} or
\mpifunc{MPI\_ERR\_PENDING}.  The call is semantically equivalent to an
\mpifunc{MPI\_COMM\_SPLIT} operation that would succeed despite failures, and
where living processes participate with the same color, and a key equal to their
rank in \textit{comm} and failed processes implicitly contribute
\mpifunc{MPI\_UNDEFINED}. 

\begin{description}

\item[Advice to Users] {This call does not guarantee that all processes in
\textit{newcomm} are alive.  Any new failure will be detected in
subsequent \mpi operations.}

\end{description}

\subsubsection*{MPI\_COMM\_FAILURE\_ACK( comm ) \\
\begin{tabular*}{1.0\textwidth}{@{\extracolsep{\fill}} l l c r }
  \texttt{IN} & \texttt{comm} & & communicator (handle) \\
\end{tabular*}}

This local operation gives the users a way to \emph{acknowledge} all locally notified
failures on \textit{comm}. After the call, unmatched \mpifunc{MPI\_ANY\_SOURCE} receptions
that would have raised an error code \mpifunc{MPI\_ERR\_PENDING} due to process failure (see
Section~\ref{sec:ft-notification:p2p-coll-comm}) proceed without further reporting
of errors due to those acknowledged failures.

\begin{description}

\item[Advice to Users] {Calling \mpifunc{MPI\_COMM\_FAILURE\_ACK} on a communicator
with failed processes does not allow that communicator to be used
successfully for collective operations.  Collective communication on a
communicator with acknowledged failures will continue to raise an error
of class \mpifunc{MPI\_ERR\_PROC\_FAILED} as defined in
Section~\ref{sec:ft-notification:p2p-coll-comm}. To reliably use collective
operations on a communicator with failed processes, the communicator
should first be revoked using \mpifunc{MPI\_COMM\_REVOKE} and then a new
communicator should be created using \mpifunc{MPI\_COMM\_SHRINK}.}

\end{description}

\subsubsection*{MPI\_COMM\_FAILURE\_GET\_ACKED( comm, failedgroup ) \\
\begin{tabular*}{1.0\textwidth}{@{\extracolsep{\fill}} l l c r } 
  \texttt{IN} & \texttt{comm} & & communicator (handle) \\
  \texttt{OUT} & \texttt{failedgroup} & & group (handle) \\
\end{tabular*}}

 This local operation returns the group \textit{failedgrp} of processes,
from the communicator \textit{comm}, which have been locally acknowledged as
failed by preceding calls to \mpifunc{MPI\_COMM\_FAILURE\_ACK}. The
failedgrp can be empty, that is, equal to \mpifunc{MPI\_GROUP\_EMPTY}.

\subsubsection*{MPI\_COMM\_AGREE( comm, flag ) \\
\begin{tabular*}{1.0\textwidth}{@{\extracolsep{\fill}} l l c r}
	\texttt{IN} & \texttt{comm} & & communicator (handle) \\
	\texttt{INOUT} & \texttt{flag} & & boolean flag \\
\end{tabular*}}

This function performs a collective operation on the group of living processes
in \textit{comm}.  On completion, all living processes must agree to set the
output value of \textit{flag} to the result of a logical \textit{'AND'}
operation over the input values of \textit{flag}. This
function must not raise an error due to process failure (error classes
\mpifunc{MPI\_ERR\_PROC\_FAILED} and \mpifunc{MPI\_ERR\_REVOKED}),
and processes that failed before entering the call do not contribute to the
operation.

If \textit{comm} is an intercommunicator, the value of \textit{flag} 
is a logical \textit{'AND'} operation over the values contributed by 
the remote group (where failed processes do not contribute to the operation).

\begin{description}

\item[Advice to Users] {\mpifunc{MPI\_COMM\_AGREE} maintains its collective behavior even if the \textit{comm} is revoked.}

\end{description}

\subsubsection*{MPI\_COMM\_IAGREE( comm, flag, req ) \\
\begin{tabular*}{1.0\textwidth}{@{\extracolsep{\fill}} l l c r}
	\texttt{IN} & \texttt{comm} & & communicator (handle) \\
	\texttt{INOUT} & \texttt{flag} & & boolean flag \\
	\texttt{OUT} & \texttt{req} & & request (handle) \\
\end{tabular*}}

This function has the same semantics as \mpifunc{MPI\_COMM\_AGREE} except that it is nonblocking.

\appendixsubsection{One-Sided Functions}
\label{sec:ft-functions:winfunctions}

\subsubsection*{MPI\_WIN\_REVOKE ( win ) \\
\begin{tabular*}{1.0\textwidth}{@{\extracolsep{\fill}} l l c r }
  \texttt{IN} & \texttt{win} & & window (handle) \\
\end{tabular*}}

 This function notifies all processes within the window \textit{win} that
this window is now considered revoked. A revoked window completes any non-local \mpi
operations on \textit{win} with error and causes any new operations to complete
with error. Once a window has been revoked, all
subsequent non-local operations on that window are considered local and must
fail with an error of class \mpifunc{MPI\_ERR\_REVOKED}.

\subsubsection*{MPI\_WIN\_GET\_FAILED( win, failedgroup ) \\
\begin{tabular*}{1.0\textwidth}{@{\extracolsep{\fill}} l l c r }
  \texttt{IN} & \texttt{win} & & window (handle) \\
  \texttt{OUT} & \texttt{failedgroup} & & group (handle) \\
\end{tabular*}}

This local operation returns the group \textit{failedgrp} of processes from the
window \textit{win} which are locally known to have failed.

\begin{description} 

\item[Advice to Users] {\mpi makes no assumption about asynchronous
progress of the failure detection. A valid \mpi implementation may choose
to only update the group of locally known failed processes when it enters
a synchronization function.}

\item[Advice to Users] {It is possible that only the calling process has
detected the reported failure. If global knowledge is necessary,
processes detecting failures should use the call \mpifunc{MPI\_WIN\_REVOKED}.}

\end{description}

\appendixsubsection{I/O Functions}
\label{sec:ft-functions:filefunctions}

\subsubsection*{MPI\_FILE\_REVOKE ( fh ) \\
\begin{tabular*}{1.0\textwidth}{@{\extracolsep{\fill}} l l c r }
  \texttt{IN} & \texttt{fh} & & file (handle) \\
\end{tabular*}}

 This function notifies all ranks within file \textit{fh} that this file
handle is now considered revoked. 


Ongoing non-local completion operations on a revoked file handle raise an exception of class
\mpifunc{MPI\_ERR\_REVOKED}. Once a file handle has been revoked, all
subsequent non-local operations on the file handle must raise an \mpi exception
of class \mpifunc{MPI\_ERR\_REVOKED}.

\appendixsection{Error Codes and Classes}
\label{sec:ft-errorcodes}

The following error classes are added to those defined in Section 8.4:
\begin{table}[htb]
\begin{center}
\captionof{table}{Additional process fault tolerance error classes}
\label{table:ft-errorcodes:errclasses}
\begin{tabular}{l p{2.8in}}
	\mpifunc{MPI\_ERR\_PROC\_FAILED} & The operation could not complete because of a process failure (a fail-stop failure). \\
    \mpifunc{MPI\_ERR\_REVOKED} & The communication object used in the operation has been revoked. \\
\end{tabular}
\end{center}
\end{table}

\appendixsection{Examples}

\appendixsubsection{Master/Worker}

The example below presents a master code that handles failures
by ignoring failed processes and resubmitting requests. It 
demonstrates the different failure cases that may occur when
posting receptions from \mpifunc{MPI\_ANY\_SOURCE} as discussed in the 
advice to users in Section~\ref{sec:ft-notification:p2p-coll-comm}.

\begin{lstlisting}[language=C,basicstyle=\ttfamily]
int master(void)
{
  MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);
  MPI_Comm_size(comm, &size);

  /* .. submit the initial work requests .. */

  MPI_Irecv( buffer, 1, MPI_INT, MPI_ANY_SOURCE, 
             tag, comm, &req );

  /* Progress engine: Get answers, send new requests, 
     and handle process failures */
  while( (active_workers > 0) && work_available ) {
    rc = MPI_Wait( &req, &status );
        
    if( (MPI_ERR_PROC_FAILED == rc) || 
        (MPI_ERR_PENDING == rc) ) {
      MPI_Comm_failure_ack(comm);
      MPI_Comm_failure_get_acked(comm, &g);
      MPI_Group_size(g, &gsize);

      /* .. find the lost work and requeue it .. */
            
      active_workers = size - gsize - 1;
      MPI_Group_free(&g);
            
      /* repost the request if it
       * matched the failed process */
      if( rc == MPI_ERR_PROC_FAILED )
        MPI_Irecv( buffer, 1, MPI_INT, MPI_ANY_SOURCE, 
                   tag, comm, &req );
      }
            
      continue;
    }

    /* .. process the answer and update work_available .. */
    MPI_Irecv( buffer, 1, MPI_INT, MPI_ANY_SOURCE, 
               tag, comm, &req );
  }

  /* .. cancel request and cleanup .. */
}
\end{lstlisting}

\appendixsubsection{Iterative Refinement}

The example below demonstrates a method of fault-tolerance to detect and handle
failures.  At each iteration, the algorithm checks the return code of the
\mpifunc{MPI\_ALLREDUCE}. If the return code indicates a process failure for at
least one process, the algorithm revokes the
communicator, agrees on the presence of failures, and later shrinks it to create
a new communicator. By calling
\mpifunc{MPI\_COMM\_REVOKE}, the algorithm
ensures that all processes will be notified of process failure and enter the
\mpifunc{MPI\_COMM\_AGREE}. If a process fails, the algorithm must complete
at least one more iteration to ensure a correct answer.

\begin{lstlisting}[language=C,basicstyle=\ttfamily]
while( gnorm > epsilon ) {
  /* Add a computation iteration to converge and 
     compute local norm in lnorm */
  rc = MPI_Allreduce( &lnorm, &gnorm, 1, 
    	     MPI_DOUBLE, MPI_MAX, comm);
		
  if( (MPI_ERR_PROC_FAILED == rc ) ||
      (MPI_ERR_COMM_REVOKE == rc) ||
      (gnorm <= epsilon) ) {
			
    if( MPI_ERR_PROC_FAILED == rc )
        MPI_Comm_revoke(comm);
			
    /* About to leave: let's be sure that everybody 
       received the same information */
    allsucceeded = (rc == MPI_SUCCESS);
    MPI_Comm_agree(comm, &allsucceeded);
    if( !allsucceeded ) {
    
      /* We plan to join the shrink, thus the 
         communicator should be marked as revoked */           
      MPI_Comm_revoke(comm);
      MPI_Comm_shrink(comm, &comm2);
            
      /* Release the revoked communicator */
      MPI_Comm_free(comm); 
      comm = comm2;
            
      /* Force one more iteration */
      gnorm = epsilon + 1.0;
    }
  }
}
\end{lstlisting}

